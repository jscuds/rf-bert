{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9212dc-c452-48fd-99c8-b721ef0b179f",
   "metadata": {},
   "source": [
    "# Debugging retrofitting code\n",
    "\n",
    "We're trying to figure out why our results from retrofitting on Sampled Quora and fine-tuning on SST-2 and MR aren't showing the same gains that are in the [Retrofitting Paper](https://aclanthology.org/D19-1113.pdf). We've noticed one key difference, in the embedding distances listed in Table 4. \n",
    "- Their table shows average L2 distances between shared words as 3.2 for paraphrases and 4.2 for non-paraphrases before retrofitting, and 1.3 for paraphrases and 5.5 for non-paraphrases (after retrofitting on all three datasets). \n",
    "- Our experiments ([example run](https://wandb.ai/jack-morris/rf-bert/runs/eqa5zall?workspace=user-jxmorris12)) show embedding distances of 6.5 for paraphrases and 10.5 for non-paraphrases, both of which are greatly increasing throughout training. Why is this the case? \n",
    "\n",
    "## This notebook\n",
    "In this notebook I want to:\n",
    "1. Figure out how to get the 3.2 and 4.2 numbers on Quora. We should be able to reproduce this exactly using the pre-trained ELMO model.\n",
    "2. Diagnose why these numbers are miscomputed in our setup. Are we getting the wrong representations? Or are we feeding in the wrong input? Or are we getting the words at the wrong index somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0de18-e5c8-4bd4-8245-68795ff733b9",
   "metadata": {},
   "source": [
    "# Computing L2 distances of base ELMO between words from Quora\n",
    "\n",
    "## Using our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c579c5ef-550a-47a8-843d-9ccf102bcee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quora (/home/jxm3/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6c814fc1474260b40d56b78b09b1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/jxm3/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04/cache-2d905bd4aa27bedf.arrow\n",
      "Processing quora paraphrases from train split:   0%|          | 1210/404290 [00:01<06:17, 1066.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pretend we're in the root folder of this project ('retrofitting/')\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "# 1. Get a batch of examples from Quora with shared words, etc.\n",
    "from dataloaders import ParaphraseDatasetElmo\n",
    "\n",
    "dataset = ParaphraseDatasetElmo(\n",
    "    'quora',\n",
    "    model_name='elmo', num_examples=1024, \n",
    "    max_length=40, stop_words_file=f'../stop_words_en.txt',\n",
    "    r1=0.5, seed=42, split='train'\n",
    ")\n",
    "\n",
    "from dataloaders.helpers import train_test_split\n",
    "train_dataloader, test_dataloader = train_test_split(\n",
    "    dataset, batch_size=256, \n",
    "    shuffle=True, drop_last=True, \n",
    "    train_split=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f05fd5e-1d1d-4439-9f51-804cb0a5e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import ElmoRetrofit\n",
    "\n",
    "# 2. Load batch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch = tuple(t.to(device) for t in next(iter(train_dataloader)))\n",
    "\n",
    "# 3. Load ELMO\n",
    "model = ElmoRetrofit(\n",
    "    num_output_representations = 1, \n",
    "    requires_grad=False,\n",
    "    elmo_dropout=0,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d483ee62-bb0b-4221-ad28-3556fbd981d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 1024]),\n",
       " torch.Size([256, 1024]),\n",
       " torch.Size([256, 1024]),\n",
       " torch.Size([256, 1024]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Do inference\n",
    "word_rep_pos_1, word_rep_pos_2, word_rep_neg_1, word_rep_neg_2 = (\n",
    "            model(*batch)\n",
    ")\n",
    "\n",
    "word_rep_pos_1.shape, word_rep_pos_2.shape, word_rep_neg_1.shape, word_rep_neg_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ea2ba-9ef6-4b41-b1aa-5e32e47ec418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2norm(t1: torch.Tensor, t2: torch.Tensor):\n",
    "    (t1 - t2).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "781901e1-52c8-410a-baec-4a95e166244e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.0903, 11.9857,  4.4868, 11.6699,  2.0959, 10.3926,  6.0436, 12.3582,\n",
       "         8.0479,  7.8047,  3.0164,  6.1769, 13.5050, 10.5956,  6.0174, 12.9105,\n",
       "         7.4838,  9.4541,  8.4242,  6.7361, 14.9254,  9.7438, 10.2964,  8.6891,\n",
       "         7.2909,  9.8680,  6.3273,  7.4691, 11.6232,  9.3006,  8.7581,  2.9304,\n",
       "         2.0199,  7.1777,  5.6352,  7.0379,  4.8691, 11.9118,  9.8337, 11.8965,\n",
       "        15.3541,  7.8360,  4.3216,  4.0533,  7.3775,  4.6901, 14.7758,  5.5396,\n",
       "         6.5728,  2.6704,  3.4293,  3.5504,  5.2220,  0.5899,  7.3558,  5.0385,\n",
       "         5.5747,  6.6362,  1.6621,  2.6443,  3.1779,  6.2965,  5.7336,  8.6673,\n",
       "         4.4809, 11.2629,  1.1161,  8.8685,  3.0433,  9.4950,  3.0517,  1.8758,\n",
       "         4.4690,  4.1822,  0.7343,  2.9548,  8.2923,  6.6806,  4.3832,  7.5175,\n",
       "         1.2673,  9.3551,  6.5259,  7.7451,  4.8253,  4.6596,  7.5497,  6.4654,\n",
       "         3.6612,  4.0151,  2.8291, 10.1500,  2.2860,  6.7031, 10.5323, 11.5086,\n",
       "         6.7557,  3.2534, 10.3592,  5.5454, 11.5359,  2.4687, 13.8914,  7.2484,\n",
       "         8.0152,  6.1026,  6.4954,  2.2313,  9.1465,  5.2008,  4.6748,  7.1238,\n",
       "         4.6162,  8.9708, 10.0469,  2.5305,  6.2735,  5.4140,  4.6227,  3.8437,\n",
       "         9.9434, 10.8198, 14.6088,  7.2126,  7.5996,  4.0203,  6.1686,  4.2312,\n",
       "         6.8118,  5.0008,  9.7157,  2.9523,  4.0344, 17.4345,  4.3001,  7.1199,\n",
       "         3.0993,  6.5291, 12.9001,  7.6930,  3.2646,  7.0819,  5.8842,  5.7551,\n",
       "         5.0181,  0.4020,  5.2961,  9.6204,  7.2892,  5.5258,  4.0990,  9.5660,\n",
       "         5.5203,  5.2902,  1.0055,  5.7076, 12.8635,  5.9788, 12.0605,  5.8942,\n",
       "         2.6899,  5.1559,  8.7209,  5.0731,  5.3959,  7.8515, 11.4244, 14.9090,\n",
       "        12.3022,  6.3252,  7.2702,  6.0938, 11.3976,  9.6865,  5.4754, 13.5781,\n",
       "         2.2560,  2.4011,  9.2080,  6.1993,  9.4045,  6.2586, 11.2991,  9.3980,\n",
       "         1.6808,  9.4475, 10.7261, 10.5628, 12.6985,  6.2585,  4.5898,  4.6058,\n",
       "         3.4538, 11.1678,  4.2404,  8.1193,  3.3708,  3.4093,  3.5135, 13.0358,\n",
       "         3.7229, 10.5458, 11.4058,  3.3804,  7.6558,  3.0449,  7.2357, 14.4645,\n",
       "        11.7209,  4.8814,  5.7491,  1.5761,  1.4849, 11.8203,  7.8078,  3.8330,\n",
       "         3.3715,  8.9719,  6.2293,  3.0730,  3.4105,  2.7960, 12.1743,  7.2904,\n",
       "         3.2917,  9.9314,  3.0396,  4.1707,  3.2494,  5.3235,  1.7871,  7.7304,\n",
       "        11.9101, 10.3115,  2.6697,  9.1831,  4.8002,  3.3579,  2.4756, 12.8089,\n",
       "        11.5962, 12.2740,  1.5326, 13.3322,  6.6252,  7.6693,  4.3061,  5.8432,\n",
       "         9.4852, 10.3695,  4.4770,  3.7318, 10.7514,  4.8632, 12.8473,  4.5862],\n",
       "       device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(word_rep_pos_1 - word_rep_pos_2, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e317236-e2c0-427f-b4e2-e9f246a09236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9159, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(word_rep_pos_1 - word_rep_pos_2, p=2, dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a7ca4a1-3b6c-487b-bf07-b0f504bdd16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.3434, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(word_rep_neg_1 - word_rep_neg_2, p=2, dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67cc294-b25d-4edd-a17b-ee20fa1f89c0",
   "metadata": {},
   "source": [
    "Ok, the distances with our code look like the ones in W&B (7 and 11) instead of the ones from the paper (3 and 4ish). I'm going to strip this down to just strings and the pytorch ELMO model and build back up, since I'm not sure where things are going wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b06d50-44e2-40c7-b838-77686369a96b",
   "metadata": {},
   "source": [
    "## Trying with original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3aa4f829-920a-4435-aa16-dbe0abc726ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quora (/home/jxm3/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca05de189694e15a82e3889325506f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef00f59d08f434b9b9ece5939e42210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4a2d6eb2614a38857e32127e5040d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(742, 1258)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "d = datasets.load_dataset('quora')['train']\n",
    "d = datasets.Dataset.from_dict(d[:2_000])\n",
    "paraphrases = d.filter(lambda ex: ex['is_duplicate'])\n",
    "non_paraphrases = d.filter(lambda ex: not ex['is_duplicate'])\n",
    "\n",
    "len(paraphrases), len(non_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fb83e3-a603-42f8-9577-3d573704308f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': {'id': [11, 12],\n",
       "  'text': ['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?',\n",
       "   \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\"]},\n",
       " 'is_duplicate': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8b23988-b302-4a93-a71c-08a960d44a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "import numpy as np\n",
    "\n",
    "def get_stop_words(filename: str) -> Set[int]:\n",
    "    \"\"\"\n",
    "    Generates a set of token_ids given a text file of stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = np.genfromtxt(filename, dtype='str')\n",
    "    return set(stop_words.tolist())\n",
    "\n",
    "sw = get_stop_words('../stop_words_en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4628c5b7-31c3-47c3-abf1-7a8a63107736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mosestokenizer import MosesTokenizer\n",
    "\n",
    "tokenizer = MosesTokenizer('en', no_escape=True)\n",
    "\n",
    "def get_shared_word(q1: List[str], q2: List[str]) -> str:\n",
    "    \"\"\"gets a shared word that's not a stopword.\n",
    "    \n",
    "    takes list of tokens as input\n",
    "    \n",
    "    raises StopIteration when no shared words found\n",
    "    \"\"\"\n",
    "    w1 = set([w for w in q1 if w.lower() not in sw])\n",
    "    w2 = set([w for w in q2 if w.lower() not in sw])\n",
    "    # print(w1, '//', w2)\n",
    "    shared_words = w1.intersection(w2)\n",
    "    return next(iter(shared_words))\n",
    "\n",
    "\n",
    "paraphrases_and_words = []\n",
    "nonparaphrases_and_words = []\n",
    "for ex in paraphrases:\n",
    "    q1 = tokenizer(ex['questions']['text'][0])\n",
    "    q2 = tokenizer(ex['questions']['text'][1])\n",
    "    try:\n",
    "        shared_word = get_shared_word(q1, q2)\n",
    "        i1 = q1.index(shared_word)\n",
    "        i2 = q2.index(shared_word)\n",
    "        paraphrases_and_words.append( (q1, q2, shared_word, i1, i2) )\n",
    "    except StopIteration:\n",
    "        continue\n",
    "        # no shared word, just continue?\n",
    "        # print(ex['questions']['text'][0])\n",
    "        # print(ex['questions']['text'][1])\n",
    "        # print()\n",
    "for ex in non_paraphrases:\n",
    "    q1 = tokenizer(ex['questions']['text'][0])\n",
    "    q2 = tokenizer(ex['questions']['text'][1])\n",
    "    try:\n",
    "        shared_word = get_shared_word(q1, q2)\n",
    "        i1 = q1.index(shared_word)\n",
    "        i2 = q2.index(shared_word)\n",
    "        if (i1 >= 40) or (i2 >= 40):\n",
    "            raise StopIteration\n",
    "        nonparaphrases_and_words.append( (q1, q2, shared_word, i1, i2) )\n",
    "    except StopIteration:\n",
    "        continue\n",
    "        # no shared word, just continue?\n",
    "        # print(ex['questions']['text'][0])\n",
    "        # print(ex['questions']['text'][1])\n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e938a9b5-9d82-4a23-a759-c193ea3320d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1_tokens</th>\n",
       "      <th>q2_tokens</th>\n",
       "      <th>shared_word</th>\n",
       "      <th>i1</th>\n",
       "      <th>i2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[What, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>[What, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>market</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[What, is, the, story, of, Kohinoor, (, Koh, @...</td>\n",
       "      <td>[What, would, happen, if, the, Indian, governm...</td>\n",
       "      <td>Noor</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[How, can, I, increase, the, speed, of, my, in...</td>\n",
       "      <td>[How, can, Internet, speed, be, increased, by,...</td>\n",
       "      <td>speed</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Which, one, dissolve, in, water, quikly, suga...</td>\n",
       "      <td>[Which, fish, would, survive, in, salt, water, ?]</td>\n",
       "      <td>water</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Motorola, (, company, ), :, Can, I, hack, my,...</td>\n",
       "      <td>[How, do, I, hack, Motorola, DCX3400, for, fre...</td>\n",
       "      <td>hack</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           q1_tokens  \\\n",
       "0  [What, is, the, step, by, step, guide, to, inv...   \n",
       "1  [What, is, the, story, of, Kohinoor, (, Koh, @...   \n",
       "2  [How, can, I, increase, the, speed, of, my, in...   \n",
       "3  [Which, one, dissolve, in, water, quikly, suga...   \n",
       "4  [Motorola, (, company, ), :, Can, I, hack, my,...   \n",
       "\n",
       "                                           q2_tokens shared_word  i1  i2  \n",
       "0  [What, is, the, step, by, step, guide, to, inv...      market  11  11  \n",
       "1  [What, would, happen, if, the, Indian, governm...        Noor  11  15  \n",
       "2  [How, can, Internet, speed, be, increased, by,...       speed   5   3  \n",
       "3  [Which, fish, would, survive, in, salt, water, ?]       water   4   6  \n",
       "4  [How, do, I, hack, Motorola, DCX3400, for, fre...        hack   7   3  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "p_df = pd.DataFrame(paraphrases_and_words, columns=['q1_tokens', 'q2_tokens', 'shared_word', 'i1', 'i2'])\n",
    "p_df.head()\n",
    "\n",
    "n_df = pd.DataFrame(nonparaphrases_and_words, columns=['q1_tokens', 'q2_tokens', 'shared_word', 'i1', 'i2'])\n",
    "n_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3d9e38e0-b7ae-46ac-83e1-69ad578aa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import batch_to_ids\n",
    "\n",
    "# paraphrase IDs\n",
    "p_q1_ids = batch_to_ids(p_df['q1_tokens'])\n",
    "p_q2_ids = batch_to_ids(p_df['q2_tokens'])\n",
    "\n",
    "# non paraphrase IDs\n",
    "n_q1_ids = batch_to_ids(n_df['q1_tokens'])\n",
    "n_q2_ids = batch_to_ids(n_df['q2_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b7a1e051-d82b-437c-ad4e-6da264240eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# using jack scudder's trick here. should be a way to do it with torch.select() though?\n",
    "B_p = torch.arange(len(p_q1_ids))\n",
    "p_q1_shared_word_ids = p_q1_ids[B_p, p_df['i1']]\n",
    "p_q2_shared_word_ids = p_q2_ids[B_p, p_df['i2']]\n",
    "\n",
    "B_n = torch.arange(len(n_q1_ids))\n",
    "n_q1_shared_word_ids = n_q1_ids[B_n, n_df['i1']]\n",
    "n_q2_shared_word_ids = n_q2_ids[B_n, n_df['i2']]\n",
    "\n",
    "# make sure shared word indices are the same - this should print True!\n",
    "print(torch.all(p_q1_shared_word_ids == p_q2_shared_word_ids, 0).all())\n",
    "print(torch.all(n_q1_shared_word_ids == n_q2_shared_word_ids, 0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "658ea25f-f7c8-4423-8c28-a9458cea36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo\n",
    "\n",
    "# options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "# weights_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for big elmo, 96m\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"\n",
    "weights_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"\n",
    "\n",
    "elmo_model = Elmo(options_file=options_file, weight_file=weights_file,\n",
    "                         num_output_representations=1,\n",
    "                         requires_grad=True, dropout=0.5,\n",
    "                         scalar_mix_parameters=[1, 1, 1]).to(device)\n",
    "\n",
    "\n",
    "# for param_name, param in elmo_model.named_parameters():\n",
    "# print(param_name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ddd03b26-d089-4858-bca1-5604a8cc551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1037, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "elmo_model.eval()\n",
    "with torch.no_grad():\n",
    "    # print(q1_reps.keys()) # dict_keys(['elmo_representations', 'mask'])\n",
    "    # print(len(q1_reps['elmo_representations'])) # list of length 1\n",
    "    # print(q1_reps['elmo_representations'][0].shape) # [727, 33, 1024]\n",
    "    B = torch.arange(len(p_q1_ids))\n",
    "    p_q1_reps = elmo_model(p_q1_ids.to(device))['elmo_representations'][0]\n",
    "    p_q1_shared_word_reps = p_q1_reps[B, p_df['i1']] # [727, 1024]\n",
    "    p_q2_reps = elmo_model(p_q2_ids.to(device))['elmo_representations'][0]\n",
    "    p_q2_shared_word_reps = p_q2_reps[B, p_df['i2']]\n",
    "    B = torch.arange(len(n_q1_ids))\n",
    "    n_q1_reps = elmo_model(n_q1_ids.to(device))['elmo_representations'][0]\n",
    "    n_q1_shared_word_reps = n_q1_reps[B, n_df['i1']] # [727, 1024]\n",
    "    n_q2_reps = elmo_model(n_q2_ids.to(device))['elmo_representations'][0]\n",
    "    n_q2_shared_word_reps = n_q2_reps[B, n_df['i2']]\n",
    "\n",
    "print( torch.norm(p_q1_shared_word_reps - p_q2_shared_word_reps, p=2, dim=1).mean() ) # I want this to be like 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7b802848-3e1d-4cf2-90b8-0de218b87e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2604, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print( torch.norm(n_q1_shared_word_reps - n_q2_shared_word_reps, p=2, dim=1).mean() ) # I want this to be 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "42875741-4647-4ca0-bc00-d53a531a86ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.3866,  5.2514,  5.7488,  4.0877,  4.5780,  5.3595,  6.7702,  7.6168,\n",
       "         6.4067,  5.8559,  7.0911,  4.0631,  4.5830,  3.6356,  4.4798,  5.1930,\n",
       "         8.0099,  3.8608,  5.9451,  5.6183,  4.9786,  5.1927,  5.7891,  5.1432,\n",
       "         6.2030,  3.2640,  8.1926,  5.4382,  7.1653,  2.3568,  3.5177,  7.3418,\n",
       "         6.6529,  3.7434,  4.1851,  3.0591,  5.9829,  6.0551,  6.0032,  6.3727,\n",
       "         5.0526,  4.3018,  5.8765,  4.2141,  8.9988,  5.2652,  7.7503,  7.7325,\n",
       "         5.5773,  6.0714,  4.5966,  6.5196,  6.2543,  6.8922,  6.2218,  3.7571,\n",
       "         3.8357,  5.3545,  5.9070,  5.2146,  2.6279,  5.7486, 10.3467,  6.2597,\n",
       "         5.7689,  6.3712,  2.7973,  4.0377,  3.5416,  5.4559,  3.1436,  4.2346,\n",
       "         6.8796,  6.1883,  4.5871,  6.5422,  6.0582,  5.5833,  8.1413,  6.8493,\n",
       "         4.3331,  4.6201,  3.1930,  5.7083,  6.2901,  5.7076,  5.1876,  4.9144,\n",
       "         6.2043,  3.9570,  5.9078,  7.3960,  4.3778,  4.4836,  5.5969,  8.7601,\n",
       "         6.1051,  4.2525,  4.0726,  5.8544,  7.2361,  3.7377,  5.3334,  5.7014,\n",
       "         4.1218,  4.8496,  5.4280,  5.2688,  6.3771,  6.9292,  8.1909,  6.1962,\n",
       "         4.3160,  5.3195,  3.5364,  6.0951,  5.9446,  5.1977,  4.1645,  6.2493,\n",
       "         5.8678,  4.3386,  6.1216,  4.3855,  4.8311,  8.3769,  4.3936,  6.0600,\n",
       "         4.7690,  3.6705,  5.5183,  5.5405,  3.4079,  6.2653,  6.5492,  4.2241,\n",
       "         5.7777,  5.2348,  8.8558,  7.6801,  5.9537,  7.6634,  6.3856,  5.2520,\n",
       "         5.2867,  6.2663,  4.8088,  5.2713,  5.5998,  4.8740,  6.0144,  6.0630,\n",
       "         5.5979,  6.1541,  4.8636,  6.1308,  4.8077,  4.8698,  5.8365,  5.4443,\n",
       "         5.2124,  3.8758,  4.9439,  4.4664,  5.0601,  5.0890,  6.6006,  5.0912,\n",
       "         7.7962,  7.9339,  5.4843,  5.2606,  5.8339,  4.9609,  5.0243,  4.7413,\n",
       "         3.5968, 12.5943,  4.0874,  6.0884,  6.8429,  6.0373,  8.9609,  5.3659,\n",
       "         6.8493,  4.8792,  5.5204,  8.0196,  3.4665,  4.0883,  3.3829,  5.0272,\n",
       "         2.1394,  5.6064,  3.7173,  5.9651,  5.2399,  3.4880,  5.8777,  3.9891,\n",
       "         4.0640,  3.9550,  7.0577,  5.4788, 10.9547,  6.4584,  6.1370,  5.1212,\n",
       "         5.6980,  7.4931,  3.4839,  6.2173,  7.1564,  7.1437,  6.1055,  5.6572,\n",
       "         6.6930,  6.4977,  5.0389,  4.1441,  8.4305,  6.3470,  4.5623,  7.3838,\n",
       "         4.5378,  4.8518,  5.6912,  4.8368,  5.4976,  4.9625,  7.2040,  5.0896,\n",
       "         3.1203,  4.3148,  6.5900,  4.2934,  6.0648,  6.8228,  6.4457,  5.7671,\n",
       "         5.1325,  4.9908,  4.9136,  4.2532,  3.9815,  6.4088,  4.7143,  9.8059,\n",
       "         9.1771,  8.6831,  7.0139,  8.1963,  5.6161,  4.7767,  5.3836,  6.3772,\n",
       "         6.6489,  5.7488,  5.8789,  5.6584,  6.8201,  5.0510,  4.2157,  8.0955,\n",
       "         3.7730,  4.4324,  4.3485,  5.1598,  6.2335,  5.4690,  5.3029,  4.8770,\n",
       "         2.9300,  7.1888,  4.0806,  4.3933,  5.8945,  6.9440,  5.2386,  8.9245,\n",
       "         5.1672,  9.6047,  3.6640,  2.7228,  3.2521,  7.9514,  6.8964,  7.0492,\n",
       "         5.1698,  3.3354,  6.5164,  3.2577,  4.9575,  5.7438,  5.0632,  7.3274,\n",
       "         4.9256,  3.7232,  6.2248,  3.8494,  8.2578,  4.7437,  5.7804,  3.8825,\n",
       "         3.0008,  2.8165,  7.0340,  4.6952,  4.5148,  6.6837,  7.1990,  4.5447,\n",
       "         6.4959,  6.3083,  4.7050,  4.6121,  5.4871,  8.2396,  4.1923,  5.0187,\n",
       "         8.1656,  5.6420,  6.7800,  5.3601,  6.1434,  3.7090,  3.9504,  8.8929,\n",
       "         6.1385,  3.9201,  4.2469,  9.5888,  4.1377,  3.8810,  5.9497,  5.7254,\n",
       "         3.8491,  3.5430,  4.2154,  4.7977,  2.9865,  5.2301,  7.6720,  5.7857,\n",
       "         8.2745,  4.3881,  9.7948,  4.2282,  5.9020,  4.5455,  5.2482,  7.1877,\n",
       "         3.7402,  5.5858,  3.7409,  3.0365,  4.5598,  3.4782,  5.8865,  5.2701,\n",
       "         3.5099,  4.2460,  3.1686,  6.1743,  5.0389,  6.5194,  5.2426,  7.0145,\n",
       "         5.3782,  5.4413,  3.6651,  3.8788,  3.5886,  5.4162,  5.7096,  3.7991,\n",
       "         5.5418,  6.1503,  7.2752,  7.0149,  5.4682,  4.6633,  4.3825,  4.2615,\n",
       "         5.3857,  3.0117,  7.7757,  5.1792,  8.0886,  8.9040,  3.2643,  7.3247,\n",
       "         5.2848,  4.8283,  5.6271,  7.0016,  7.2116,  3.6925,  4.7407,  7.3230,\n",
       "         7.8859,  3.1142,  5.9432,  5.9915,  6.7327,  4.7787,  4.5814,  5.3699,\n",
       "         5.1665,  6.2964,  5.1617,  3.6931,  4.5620,  3.1548,  3.6820,  7.3463,\n",
       "         5.2939,  4.8624,  4.0544,  3.0464,  5.6940,  6.0033,  3.9459,  5.4431,\n",
       "         4.2317,  8.7025,  7.1935,  7.4176,  3.9054,  4.4488,  3.3997,  3.3763,\n",
       "         4.6716,  5.3226,  4.0605,  3.0052,  3.7724,  3.4672,  3.0438,  6.4798,\n",
       "         3.4656,  3.5153,  5.0178,  5.3883,  5.1417,  7.0464,  4.5312,  6.1927,\n",
       "         5.8484,  4.4459,  5.6065,  3.8218,  8.5878,  8.9478,  4.0014,  5.0030,\n",
       "         5.4925,  5.0251,  5.2069,  5.9741,  4.1830,  3.5036,  6.1703,  4.9357,\n",
       "         5.2197,  2.9283,  9.0656,  8.1007,  3.9311,  3.8254,  5.5386,  2.6333,\n",
       "         4.7264,  4.4073,  5.2111,  4.0912,  7.5193,  6.6060,  4.5528,  6.5461,\n",
       "         5.9665,  4.4084,  4.3885,  8.5070,  7.2590,  2.4511,  6.5716,  5.3681,\n",
       "         3.6556,  8.7702,  5.8882,  4.8592,  5.9371,  3.4513,  4.3050,  4.6485,\n",
       "         3.8562,  5.7629,  4.5185,  5.5360,  4.7922,  5.4859,  4.1606,  5.9042,\n",
       "         4.8968,  3.5673,  5.3622,  4.4492,  7.6516,  7.8626,  4.5916,  4.3936,\n",
       "         4.9579,  7.6923,  3.4408,  4.4169,  4.6687,  6.3639,  4.6449,  5.6509,\n",
       "         8.9741,  7.2169,  7.0605,  6.1607,  6.1399,  5.6800,  4.6591,  5.0854,\n",
       "         8.8758,  5.0045,  4.6834,  4.6921,  3.2391,  5.4677,  4.8156,  8.4009,\n",
       "         4.5964,  4.2336,  4.2499,  4.8581,  4.9531,  4.0650,  4.5249,  6.8783,\n",
       "         4.8985,  6.9000,  2.9983,  5.7027,  6.7161,  6.0892,  9.2105,  4.1133,\n",
       "         7.2197,  6.7486,  5.7730,  5.6554,  6.2171,  5.7165,  3.2363,  6.4367,\n",
       "         6.6776,  4.7428,  4.3991,  7.0508,  4.0766,  6.3029,  3.8530,  3.5381,\n",
       "         6.1799,  5.2374,  4.0198,  6.9159,  5.8424,  7.9337,  7.7644,  5.7434,\n",
       "         4.7998,  5.3684,  5.0215,  4.2793,  5.2129,  5.7573,  6.6669,  8.3751,\n",
       "         3.5960,  3.2639,  4.9086,  4.6625,  3.1322,  5.2349,  8.8354,  6.4964,\n",
       "         6.1066,  3.2267,  6.3926,  6.9309,  3.9881,  5.5815,  5.5447,  7.6763,\n",
       "         3.9117,  5.7165,  6.3910,  6.3978,  6.3372,  2.9308,  5.1764,  8.9208,\n",
       "         2.5621,  5.9300,  3.7926,  4.9667,  6.2129,  5.2807,  5.9073,  7.1600,\n",
       "         4.6117,  3.5995, 11.1127,  7.1006,  4.0780,  2.9127,  6.6785,  4.9106,\n",
       "         4.3778,  7.0403,  8.3145,  4.0336,  5.4053,  6.2803,  5.2225,  4.8180,\n",
       "         5.6335,  6.4443,  6.6574,  4.8563,  3.6777,  4.9608,  5.7354,  6.0797,\n",
       "         5.3382,  7.8101,  6.1413,  6.7385,  5.6170,  4.7289,  5.6643,  5.6627,\n",
       "         2.6013,  5.3157,  6.7416,  3.2610,  4.9539,  5.6690,  2.9637,  4.3278,\n",
       "         6.9413,  5.6281,  7.0766,  6.2421,  3.9609,  7.5577,  4.7369,  6.0357,\n",
       "         7.6562,  4.5636,  3.3329,  5.4304,  6.3483,  5.6457,  5.7781,  4.8597,\n",
       "         6.3231,  4.7199,  4.3303,  2.4389,  7.4829,  4.8594,  3.4127,  6.9080,\n",
       "         5.9243,  6.7043,  4.7758,  6.1258, 14.8145,  3.4950,  3.9406,  4.8113,\n",
       "         4.8141,  4.0342,  4.4399,  5.3683,  4.7463,  5.8784,  9.8474,  4.5551,\n",
       "         3.9061,  7.2162,  3.4785,  4.9798,  3.6892,  6.9477,  9.4851,  6.4835,\n",
       "         3.2784,  6.0797,  5.1588,  4.3730,  4.6301,  3.2396,  5.2636],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p_q1_shared_word_reps - p_q2_shared_word_reps).norm(p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2bbff756-ff01-40c7-a0a8-98b39597361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([727, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(q1_shared_word_reps.shape)\n",
    "assert q1_shared_word_reps.shape == q2_shared_word_reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "972bf793-c44b-4426-a1d2-91a195bc08e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3548, device='cuda:0')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(q1_shared_word_reps - q2_shared_word_reps, p=2, dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "545e6c47-77aa-4add-a33b-7acd6abcf582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_shared_word_reps.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8b74c-b9e6-46b8-8716-fc3631c991cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
