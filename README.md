# rf-bert

Repo for BERT implementation of "Retrofitting Contextualized Word Embeddings with Paraphrases" [[1]](#1) and further improvement of BERT robustness.

## Forthcoming Contributions
1. retrofitted-BERT in PyTorch
2. experiments to test rf-BERT robustness under different attacks




## References
<a id="1">[1]</a> 
```bibtex
@inproceedings{shi-etal-2019-retrofitting,
    title = {Retrofitting Contextualized Word Embeddings with Paraphrases},
    author = {Shi, Weijia and Chen, Muhao and Zhou, Pei and Chang, Kai-Wei},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    year = {2019},
    url = {https://aclanthology.org/D19-1113},
    pages = {1198--1203}
}
```