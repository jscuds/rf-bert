{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9212dc-c452-48fd-99c8-b721ef0b179f",
   "metadata": {},
   "source": [
    "# Debugging retrofitting code\n",
    "\n",
    "We're trying to figure out why our results from retrofitting on Sampled Quora and fine-tuning on SST-2 and MR aren't showing the same gains that are in the [Retrofitting Paper](https://aclanthology.org/D19-1113.pdf). We've noticed one key difference, in the embedding distances listed in Table 4. \n",
    "- Their table shows average L2 distances between shared words as 3.2 for paraphrases and 4.2 for non-paraphrases before retrofitting, and 1.3 for paraphrases and 5.5 for non-paraphrases (after retrofitting on all three datasets). \n",
    "- Our experiments ([example run](https://wandb.ai/jack-morris/rf-bert/runs/eqa5zall?workspace=user-jxmorris12)) show embedding distances of 6.5 for paraphrases and 10.5 for non-paraphrases, both of which are greatly increasing throughout training. Why is this the case? \n",
    "\n",
    "## This notebook\n",
    "In this notebook I want to:\n",
    "1. Figure out how to get the 3.2 and 4.2 numbers on Quora. We should be able to reproduce this exactly using the pre-trained ELMO model.\n",
    "2. Diagnose why these numbers are miscomputed in our setup. Are we getting the wrong representations? Or are we feeding in the wrong input? Or are we getting the words at the wrong index somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0de18-e5c8-4bd4-8245-68795ff733b9",
   "metadata": {},
   "source": [
    "# Computing L2 distances of base ELMO between words from Quora\n",
    "\n",
    "## Using our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c579c5ef-550a-47a8-843d-9ccf102bcee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quora (/home/jxm3/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6c814fc1474260b40d56b78b09b1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/jxm3/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04/cache-2d905bd4aa27bedf.arrow\n",
      "Processing quora paraphrases from train split:   0%|          | 1210/404290 [00:01<06:17, 1066.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pretend we're in the root folder of this project ('retrofitting/')\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "# 1. Get a batch of examples from Quora with shared words, etc.\n",
    "from dataloaders import ParaphraseDatasetElmo\n",
    "\n",
    "dataset = ParaphraseDatasetElmo(\n",
    "    'quora',\n",
    "    model_name='elmo', num_examples=1024, \n",
    "    max_length=40, stop_words_file=f'../stop_words_en.txt',\n",
    "    r1=0.5, seed=42, split='train'\n",
    ")\n",
    "\n",
    "from dataloaders.helpers import train_test_split\n",
    "train_dataloader, test_dataloader = train_test_split(\n",
    "    dataset, batch_size=256, \n",
    "    shuffle=True, drop_last=True, \n",
    "    train_split=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f05fd5e-1d1d-4439-9f51-804cb0a5e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import ElmoRetrofit\n",
    "\n",
    "# 2. Load batch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch = tuple(t.to(device) for t in next(iter(train_dataloader)))\n",
    "\n",
    "# 3. Load ELMO\n",
    "model = ElmoRetrofit(\n",
    "    num_output_representations = 1, \n",
    "    requires_grad=False,\n",
    "    elmo_dropout=0,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d483ee62-bb0b-4221-ad28-3556fbd981d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 1024]),\n",
       " torch.Size([256, 1024]),\n",
       " torch.Size([256, 1024]),\n",
       " torch.Size([256, 1024]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Do inference\n",
    "word_rep_pos_1, word_rep_pos_2, word_rep_neg_1, word_rep_neg_2 = (\n",
    "            model(*batch)\n",
    ")\n",
    "\n",
    "word_rep_pos_1.shape, word_rep_pos_2.shape, word_rep_neg_1.shape, word_rep_neg_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ea2ba-9ef6-4b41-b1aa-5e32e47ec418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2norm(t1: torch.Tensor, t2: torch.Tensor):\n",
    "    (t1 - t2).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "781901e1-52c8-410a-baec-4a95e166244e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.0903, 11.9857,  4.4868, 11.6699,  2.0959, 10.3926,  6.0436, 12.3582,\n",
       "         8.0479,  7.8047,  3.0164,  6.1769, 13.5050, 10.5956,  6.0174, 12.9105,\n",
       "         7.4838,  9.4541,  8.4242,  6.7361, 14.9254,  9.7438, 10.2964,  8.6891,\n",
       "         7.2909,  9.8680,  6.3273,  7.4691, 11.6232,  9.3006,  8.7581,  2.9304,\n",
       "         2.0199,  7.1777,  5.6352,  7.0379,  4.8691, 11.9118,  9.8337, 11.8965,\n",
       "        15.3541,  7.8360,  4.3216,  4.0533,  7.3775,  4.6901, 14.7758,  5.5396,\n",
       "         6.5728,  2.6704,  3.4293,  3.5504,  5.2220,  0.5899,  7.3558,  5.0385,\n",
       "         5.5747,  6.6362,  1.6621,  2.6443,  3.1779,  6.2965,  5.7336,  8.6673,\n",
       "         4.4809, 11.2629,  1.1161,  8.8685,  3.0433,  9.4950,  3.0517,  1.8758,\n",
       "         4.4690,  4.1822,  0.7343,  2.9548,  8.2923,  6.6806,  4.3832,  7.5175,\n",
       "         1.2673,  9.3551,  6.5259,  7.7451,  4.8253,  4.6596,  7.5497,  6.4654,\n",
       "         3.6612,  4.0151,  2.8291, 10.1500,  2.2860,  6.7031, 10.5323, 11.5086,\n",
       "         6.7557,  3.2534, 10.3592,  5.5454, 11.5359,  2.4687, 13.8914,  7.2484,\n",
       "         8.0152,  6.1026,  6.4954,  2.2313,  9.1465,  5.2008,  4.6748,  7.1238,\n",
       "         4.6162,  8.9708, 10.0469,  2.5305,  6.2735,  5.4140,  4.6227,  3.8437,\n",
       "         9.9434, 10.8198, 14.6088,  7.2126,  7.5996,  4.0203,  6.1686,  4.2312,\n",
       "         6.8118,  5.0008,  9.7157,  2.9523,  4.0344, 17.4345,  4.3001,  7.1199,\n",
       "         3.0993,  6.5291, 12.9001,  7.6930,  3.2646,  7.0819,  5.8842,  5.7551,\n",
       "         5.0181,  0.4020,  5.2961,  9.6204,  7.2892,  5.5258,  4.0990,  9.5660,\n",
       "         5.5203,  5.2902,  1.0055,  5.7076, 12.8635,  5.9788, 12.0605,  5.8942,\n",
       "         2.6899,  5.1559,  8.7209,  5.0731,  5.3959,  7.8515, 11.4244, 14.9090,\n",
       "        12.3022,  6.3252,  7.2702,  6.0938, 11.3976,  9.6865,  5.4754, 13.5781,\n",
       "         2.2560,  2.4011,  9.2080,  6.1993,  9.4045,  6.2586, 11.2991,  9.3980,\n",
       "         1.6808,  9.4475, 10.7261, 10.5628, 12.6985,  6.2585,  4.5898,  4.6058,\n",
       "         3.4538, 11.1678,  4.2404,  8.1193,  3.3708,  3.4093,  3.5135, 13.0358,\n",
       "         3.7229, 10.5458, 11.4058,  3.3804,  7.6558,  3.0449,  7.2357, 14.4645,\n",
       "        11.7209,  4.8814,  5.7491,  1.5761,  1.4849, 11.8203,  7.8078,  3.8330,\n",
       "         3.3715,  8.9719,  6.2293,  3.0730,  3.4105,  2.7960, 12.1743,  7.2904,\n",
       "         3.2917,  9.9314,  3.0396,  4.1707,  3.2494,  5.3235,  1.7871,  7.7304,\n",
       "        11.9101, 10.3115,  2.6697,  9.1831,  4.8002,  3.3579,  2.4756, 12.8089,\n",
       "        11.5962, 12.2740,  1.5326, 13.3322,  6.6252,  7.6693,  4.3061,  5.8432,\n",
       "         9.4852, 10.3695,  4.4770,  3.7318, 10.7514,  4.8632, 12.8473,  4.5862],\n",
       "       device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(word_rep_pos_1 - word_rep_pos_2, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e317236-e2c0-427f-b4e2-e9f246a09236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9159, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(word_rep_pos_1 - word_rep_pos_2, p=2, dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a7ca4a1-3b6c-487b-bf07-b0f504bdd16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.3434, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(word_rep_neg_1 - word_rep_neg_2, p=2, dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67cc294-b25d-4edd-a17b-ee20fa1f89c0",
   "metadata": {},
   "source": [
    "Ok, the distances with our code look like the ones in W&B (7 and 11) instead of the ones from the paper (3 and 4ish). I'm going to strip this down to just strings and the pytorch ELMO model and build back up, since I'm not sure where things are going wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b06d50-44e2-40c7-b838-77686369a96b",
   "metadata": {},
   "source": [
    "## Trying with original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b23988-b302-4a93-a71c-08a960d44a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Set\n",
    "import numpy as np\n",
    "\n",
    "def get_stop_words(filename: str) -> Set[int]:\n",
    "    \"\"\"\n",
    "    Generates a set of token_ids given a text file of stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = np.genfromtxt(filename, dtype='str')\n",
    "    return set(stop_words.tolist())\n",
    "\n",
    "sw = get_stop_words('../stop_words_en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4628c5b7-31c3-47c3-abf1-7a8a63107736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mosestokenizer import MosesTokenizer\n",
    "\n",
    "tokenizer = MosesTokenizer('en', no_escape=True)\n",
    "\n",
    "def get_shared_word(q1: List[str], q2: List[str]) -> str:\n",
    "    \"\"\"gets a shared word that's not a stopword.\n",
    "    \n",
    "    takes list of tokens as input\n",
    "    \n",
    "    raises StopIteration when no shared words found\n",
    "    \"\"\"\n",
    "    w1 = set([w for w in q1 if w.lower() not in sw])\n",
    "    w2 = set([w for w in q2 if w.lower() not in sw])\n",
    "    # print(w1, '//', w2)\n",
    "    shared_words = w1.intersection(w2)\n",
    "    return next(iter(shared_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48716d58-1f98-40d0-b169-cf4dda4d1d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elmo(\n",
       "  (_elmo_lstm): _ElmoBiLm(\n",
       "    (_token_embedder): _ElmoCharacterEncoder(\n",
       "      (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "      (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "      (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "      (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "      (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "      (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "      (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "      (_highways): Highway(\n",
       "        (_layers): ModuleList(\n",
       "          (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "          (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    )\n",
       "    (_elmo_lstm): ElmoLstm(\n",
       "      (forward_layer_0): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (backward_layer_0): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (forward_layer_1): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (backward_layer_1): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (scalar_mix_0): ScalarMix(\n",
       "    (scalar_parameters): ParameterList(\n",
       "        (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "        (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "        (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from allennlp.modules.elmo import Elmo\n",
    "\n",
    "# ELMO 1B, 96m\n",
    "# options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "# weights_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ELMO 5.5B, 96m\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"\n",
    "weights_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"\n",
    "\n",
    "elmo_model = Elmo(options_file=options_file, weight_file=weights_file,\n",
    "                         num_output_representations=1,\n",
    "                         requires_grad=True, dropout=0.5,\n",
    "                         scalar_mix_parameters=[1, 1, 1]).to(device)\n",
    "\n",
    "elmo_model.eval()\n",
    "# for param_name, param in elmo_model.named_parameters():\n",
    "# print(param_name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d569ef52-648c-4996-8edb-8da842aaef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import batch_to_ids\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def batched_inference(model: torch.nn.Module, data: torch.Tensor, batch_size: int=256) -> torch.Tensor:\n",
    "    i = 0\n",
    "    output = []\n",
    "    while i < len(data):\n",
    "        data_batch = data[i:i+batch_size].to(device)\n",
    "        i += batch_size\n",
    "        output.append(model(data_batch)['elmo_representations'][0].cpu())\n",
    "    return torch.cat(output, dim=0)\n",
    "\n",
    "def get_shared_word_reps(paraphrases, lower=False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    paraphrases_and_words = []\n",
    "    for ex in paraphrases:\n",
    "        q1 = ex['questions']['text'][0]\n",
    "        q2 = ex['questions']['text'][1]\n",
    "        if lower:\n",
    "            q1 = q1.lower()\n",
    "            q2 = q2.lower()\n",
    "        q1 = tokenizer(q1)\n",
    "        q2 = tokenizer(q2)\n",
    "        try:\n",
    "            shared_word = get_shared_word(q1, q2)\n",
    "            i1 = q1.index(shared_word)\n",
    "            i2 = q2.index(shared_word)\n",
    "            paraphrases_and_words.append( (q1, q2, shared_word, i1, i2) )\n",
    "        except StopIteration:\n",
    "            continue\n",
    "    p_df = pd.DataFrame(paraphrases_and_words, columns=['q1_tokens', 'q2_tokens', 'shared_word', 'i1', 'i2'])\n",
    "    p_df.head()\n",
    "    # paraphrase IDs\n",
    "    p_q1_ids = batch_to_ids(p_df['q1_tokens'])\n",
    "    p_q2_ids = batch_to_ids(p_df['q2_tokens'])\n",
    "    # using jack scudder's trick here. should be a way to do it with torch.select() though?\n",
    "    B_p = torch.arange(len(p_q1_ids))\n",
    "    p_q1_shared_word_ids = p_q1_ids[B_p, p_df['i1']]\n",
    "    p_q2_shared_word_ids = p_q2_ids[B_p, p_df['i2']]\n",
    "    # make sure shared word indices are the same - this should print True!\n",
    "    print(torch.all(p_q1_shared_word_ids == p_q2_shared_word_ids, 0).all())\n",
    "    with torch.no_grad():\n",
    "        # print(q1_reps.keys()) # dict_keys(['elmo_representations', 'mask'])\n",
    "        # print(len(q1_reps['elmo_representations'])) # list of length 1\n",
    "        # print(q1_reps['elmo_representations'][0].shape) # [727, 33, 1024]\n",
    "        B = torch.arange(len(p_q1_ids))\n",
    "        p_q1_reps = batched_inference(elmo_model, p_q1_ids)\n",
    "        p_q1_shared_word_reps = p_q1_reps[B, p_df['i1']] # [727, 1024]\n",
    "        p_q2_reps = batched_inference(elmo_model, p_q2_ids)\n",
    "        p_q2_shared_word_reps = p_q2_reps[B, p_df['i2']]\n",
    "    return p_q1_shared_word_reps, p_q2_shared_word_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd03b26-d089-4858-bca1-5604a8cc551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quora (/home/jxm3/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce0cb20d003401e89032d6e2f32329b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76255a22b7974b27a067ced7296d6c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d460203f88a4dd881b5f8f2ede90a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18649, 31351)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "d = datasets.load_dataset('quora')['train']\n",
    "d = datasets.Dataset.from_dict(d[:8_000])\n",
    "quora_paraphrases = d.filter(lambda ex: ex['is_duplicate'])\n",
    "quora_non_paraphrases = d.filter(lambda ex: not ex['is_duplicate'])\n",
    "\n",
    "len(quora_paraphrases), len(quora_non_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb02d63-06ad-445c-8686-d3d8d8b312cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.1280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_p1, q_p2 = get_shared_word_reps(quora_paraphrases, lower=False)\n",
    "# I want this to be 3.17\n",
    "# ELMO 5.5B:\n",
    "# - with uppercase letters included (lower=False): 4.122    \n",
    "# - without uppercase letters: 3.808\n",
    "# ELMO 1B:\n",
    "# - with uppercase letters included (lower=False): 6.737\n",
    "# - without uppercase letters: 6.463\n",
    "(q_p1 - q_p2).norm(p=2, dim=1).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdaac18-0d39-4c94-9067-fde4fa85a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "q_n1, q_n2 = get_shared_word_reps(quora_non_paraphrases, lower=False)\n",
    "# I want this to be 4.42\n",
    "# ELMO 5.5B:\n",
    "# - with uppercase letters included: \n",
    "# - without uppercase letters: \n",
    "# ELMO 1B:\n",
    "# - with uppercase letters included: \n",
    "# - without uppercase letters: \n",
    "(q_n1 - q_n2).norm(p=2, dim=1).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f9927a8-1bd2-44d8-a3a9-63468634d2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series((q_n1 - q_n2).norm(p=2, dim=1).cpu()).hist(bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3928abe-1266-4a5a-ad09-36c4202fcb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
